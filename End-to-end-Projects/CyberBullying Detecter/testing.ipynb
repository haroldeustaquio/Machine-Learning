{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/cyberbullying_test.csv')\n",
    "train = pd.read_csv('data/cyberbullying_train.csv')\n",
    "val = pd.read_csv('data/cyberbullying_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = 'test'\n",
    "train['target'] = 'train'\n",
    "val['target'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test,train,val],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc= string.punctuation\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].str.lower().str.strip().replace(f'[{punc}]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tweet_text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lblencoder = LabelEncoder()\n",
    "df['cyberbullying_type'] = lblencoder.fit_transform(df['cyberbullying_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaling or Normalizing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['target'] == 'train'].drop(columns='target').reset_index(drop=True)\n",
    "test = df[df['target'] == 'test'].drop(columns='target').reset_index(drop=True)\n",
    "val = df[df['target'] == 'val'].drop(columns='target').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['tweet_text'].values\n",
    "y_train = train['cyberbullying_type'].values\n",
    "\n",
    "x_test = test['tweet_text'].values\n",
    "y_test = test['cyberbullying_type'].values\n",
    "\n",
    "x_val = val['tweet_text'].values\n",
    "y_val = val['cyberbullying_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets de TensorFlow\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "raw_val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for adapt is 383.9579\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='tf-idf'\n",
    ")\n",
    "\n",
    "# Obtener el texto sin etiquetas\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "start = time.time()\n",
    "vectorize_layer.adapt(text_ds)\n",
    "print(f'Time for adapt is {time.time()-start:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "raw_train_ds = raw_train_ds.shuffle(20000).batch(batch_size)\n",
    "raw_val_ds = raw_val_ds.batch(batch_size)\n",
    "raw_test_ds = raw_test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'thankyou mr president we feel blessed  proud to hv u as our closest friend america is stronger amp safer under ur govt just like india is under narendra modi yes together we will beat the chinese virus amp the radical islamic terrorism god bless india amp america forever'\n",
      " b'yeah these cops probably did disrespect their parents bunch of school bullies probably they had to become a cop to fee empowered what losers'], shape=(2,), dtype=string) tf.Tensor([5 0], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 30000), dtype=float32, numpy=\n",
       "array([[ 0.       ,  2.467025 ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  1.3330971, ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 8.892809 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       ...,\n",
       "       [17.785618 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 8.892809 ,  1.2335125,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [17.785618 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inp, target in raw_train_ds.take(1):\n",
    "    print(inp[:2], target[:2])\n",
    "    \n",
    "vectorize_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    x = vectorize_layer(x)  # Vectorizar el texto\n",
    "    y = tf.one_hot(y, depth=6)  # Convertir las etiquetas a one-hot\n",
    "    return x,y\n",
    "\n",
    "train_ds = raw_train_ds.map(lambda x,y: preprocess(x,y))\n",
    "test_ds = raw_test_ds.map(lambda x,y: preprocess(x,y))\n",
    "val_ds = raw_val_ds.map(lambda x,y: preprocess(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 30000), dtype=float32, numpy=\n",
       " array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "          0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "          0.       ],\n",
       "        [53.356853 ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "          0.       ],\n",
       "        ...,\n",
       "        [ 0.       ,  0.       ,  1.3330971, ...,  0.       ,  0.       ,\n",
       "          0.       ],\n",
       "        [ 0.       ,  1.2335125,  1.3330971, ...,  0.       ,  0.       ,\n",
       "          0.       ],\n",
       "        [ 0.       ,  1.2335125,  1.3330971, ...,  0.       ,  0.       ,\n",
       "          0.       ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(16, 6), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.]], dtype=float32)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building and Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, num_outputs, activation=None):\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.w = self.add_weight(\n",
    "            shape=[input_dim, self.num_outputs], \n",
    "            name=\"kernel\", \n",
    "            regularizer=keras.regularizers.l2(0.01)  # Añadir regularización L2\n",
    "        )\n",
    "        self.b = self.add_weight(shape=[self.num_outputs], name=\"bias\")\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.w) + self.b  # Asegúrate de usar tf.matmul\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Model(keras.Model):\n",
    "    def __init__(self, activation, dropout_rate=0.7):  # Ajusta la tasa de dropout si es necesario\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(64, activation)  # Reducir la primera capa a 64 neuronas\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)  # Dropout después de la primera capa\n",
    "        self.l2 = Linear(6, activation='softmax')  # Capa de salida para 6 clases\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.dropout1(x)  # Aplicar Dropout\n",
    "        return self.l2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu - 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model('leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2087/2087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 53ms/step - accuracy: 0.7460 - loss: 1.0596 - val_accuracy: 0.8320 - val_loss: 0.8245\n",
      "Epoch 2/10\n",
      "\u001b[1m2087/2087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 54ms/step - accuracy: 0.8461 - loss: 0.7890 - val_accuracy: 0.8235 - val_loss: 0.8272\n",
      "Epoch 3/10\n",
      "\u001b[1m2087/2087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 54ms/step - accuracy: 0.8457 - loss: 0.7930 - val_accuracy: 0.8260 - val_loss: 0.8044\n",
      "Epoch 4/10\n",
      "\u001b[1m1426/2087\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 49ms/step - accuracy: 0.8457 - loss: 0.7734"
     ]
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reducir la tasa de aprendizaje\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,  # Aumentar las épocas\n",
    "    callbacks=[early_stopping],  # Añadir early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los pesos del mejor modelo\n",
    "model.save_weights(\"best_model.weights.h5\")  # Guarda solo los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar solo los pesos del mejor modelo\n",
    "loaded_model = Model(activation='leaky_relu')  # Crear una nueva instancia del modelo\n",
    "loaded_model.load_weights(\"best_model.weights.h5\")  # Cargar los pesos\n",
    "\n",
    "# Compilar el modelo antes de evaluarlo\n",
    "loaded_model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = loaded_model.evaluate(test_ds)\n",
    "print(f\"Loss en test: {test_loss}, Accuracy en test: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las predicciones del conjunto de prueba\n",
    "predictions = loaded_model.predict(test_ds)\n",
    "\n",
    "# Convertir las predicciones a clases\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Obtener las etiquetas verdaderas\n",
    "true_classes = np.concatenate([y for _, y in test_ds], axis=0)  # Asegúrate de que test_ds contenga los labels\n",
    "true_classes = np.argmax(true_classes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la matriz de confusión\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test['cyberbullying_type'],predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar pérdida\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "# Precisión\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de Validación')\n",
    "plt.title('Precisión durante el Entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
